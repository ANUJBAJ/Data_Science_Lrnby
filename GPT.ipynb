{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382e7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 16:53:34.751459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b52b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_btch(btch_sz):\n",
    "    ix = random.choices(np.arange(length - btch_sz),k = (btch_sz))\n",
    "    tmp1 = [data[i:i+ws] for i  in ix]\n",
    "    tmp2 = [data[i+1:i+ws+1] for i  in ix]\n",
    "    x = tf.stack(tmp1)\n",
    "    y = tf.stack(tmp2)\n",
    "    return len(tmp1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set(inp_dt,btch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inp_dt)\n",
    "    dataset = dataset.window(ws+1,shift = 1,drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(ws+1))\n",
    "    dataset = dataset.map(lambda window: (window[:ws], window[1:ws+1]))\n",
    "    dataset = dataset.shuffle(10)\n",
    "    dataset = dataset.batch(btch_size).prefetch(1)\n",
    "    for i in dataset:\n",
    "       \n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae68610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size,embd_sz)\n",
    "        self.post_embedding_table = tf.keras.layers.Embedding(ws,embd_sz)\n",
    "        self.head = tf.keras.layers.Dense(vcb_sz,activation = 'linear')\n",
    "    def call(self, inputs, targets=None):\n",
    "        tok_embds = self.token_embedding_table(inputs)\n",
    "        pos_embds = self.post_embedding_table(np.arange(8))\n",
    "        x = tok_embds+pos_embds\n",
    "        logits = self.head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = tf.reshape(logits, (B*T, C))\n",
    "            targets = tf.reshape(targets, (B*T,))\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "            idx_next = tf.random.categorical(probs, 1,dtype=tf.int32)\n",
    "            \n",
    "            inputs = tf.concat([inputs, idx_next], axis=1)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "# vocab_size = 66\n",
    "# model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# # Initialize optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# # Define your training loop\n",
    "# batch_size = 32\n",
    "# for steps in range(1):  # Increase number of steps for good results...\n",
    "#     # Sample a batch of data\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         logits, loss = model(xb, yb)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# print(loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfce908",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('input.txt','r',encoding ='utf-8' ) as f1:\n",
    "    text = f1.read()\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7527c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "global vcb_sz,ws,length,embd_sz\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "vcb_sz = len(chars)\n",
    "embd_sz = 32\n",
    "ws = 8\n",
    "btch_sz_tr = 4\n",
    "btch_sz_tst = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch : i for  i,ch in enumerate(chars)}\n",
    "itos = {i : ch for  i,ch in enumerate(chars)}\n",
    "encode = lambda str1 : [stoi[j] for j in str1]\n",
    "decode = lambda lst2 : ''.join([itos[k] for k in lst2])\n",
    "print(encode('hiithere'))\n",
    "print(decode([45, 46, 46, 57, 45,0, 42, 55, 42]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = tf.constant(encode(text),dtype = tf.int32)\n",
    "length = len(data)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splt = int(0.85*len(data))\n",
    "tr_dt = data[:splt]\n",
    "tst_dt = data[splt+1:]\n",
    "tst_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_btch(btch_sz_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#smp_dt = tf.reshape(smp_dt,shape = (btch_sz*ws,vcb_sz))\n",
    "model = BigramLanguageModel(vcb_sz)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3)\n",
    "\n",
    "# Define your training loop\n",
    "#batch_size = 32\n",
    "for steps in range(2000):  # Increase number of steps for good results...\n",
    "    # Sample a batch of data\n",
    "#     for i in model.trainable_variables:\n",
    "#         print(i)\n",
    "    xb, yb = get_btch(btch_sz_tr)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb,yb)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print(loss.numpy(),steps)\n",
    "print(decode(model.generate(inputs = tf.zeros((1, 1),dtype=tf.int32), max_new_tokens=1000).numpy().squeeze().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_arr = get_btch(1)\n",
    "\n",
    "# trg = tf.constant([0,1,2,3,4,5,6,7],shape=(1,8))\n",
    "# inp1 = tf.keras.Input(shape = (8))\n",
    "# inp2 = tf.keras.Input(shape = (8))\n",
    "# x = tf.keras.layers.Embedding(vcb_sz,32)(inp1)\n",
    "# y = tf.keras.layers.Embedding(ws,32)(inp2)\n",
    "# otpt = tf.keras.layers.Dense(66,activation = 'linear')(x+y)\n",
    "# md = tf.keras.Model(inputs = [inp1,inp2],outputs = [otpt])\n",
    "# md.compile(tf.keras.optimizers.AdamW(),tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "# md.fit([in_arr[0],trg],in_arr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfe226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(0,1,66)\n",
    "# y = np.linspace(0,1,8)\n",
    "# print(x.shape)\n",
    "# X,Y = np.meshgrid(x,y)\n",
    "# print(X.shape)\n",
    "# #levels = np.linspace(np.min(z), np.max(z), 25)\n",
    "# #fig , ax = plt.subplots(1,1)\n",
    "# levels = np.linspace(np.min(asd[0]), np.max(asd[0]), 25)\n",
    "# fig , ax = plt.subplots(1,1)\n",
    "# ax.contour(X,Y,asd[0])\n",
    "# fig,bx = plt.subplots(1,2)\n",
    "# bx = plt.axes(projection ='3d')\n",
    "\n",
    "# bx.plot_surface(X,Y,asd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentv=([1,2,3],'fg',(34,56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentv[-1].append('Poetry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c859be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [[2,1],[3,1],[4,1]]\n",
    "c = np.mean(x1,axis =1)\n",
    "d = x1 - c\n",
    "v = np.transpose(d)\n",
    "f = np.matmul(v,d)\n",
    "print(d.shape,v.shape)\n",
    "f\n",
    "#x4 = tf.stack([x1,x2],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6df9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [[2,3,4],[1,1,1]]\n",
    "c = np.mean(x1,axis =0)\n",
    "\n",
    "d = x1 - c\n",
    "v = np.transpose(d)\n",
    "f = np.matmul(v,d)\n",
    "print(d.shape,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = lambda x : x**2\n",
    "asd(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4723efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array([1,2,3,4])\n",
    "x3 = np.array([1,0,1,2])\n",
    "\n",
    "mat = (np.stack([x2,x3],axis =1 ))\n",
    "print(mat)\n",
    "mat  = np.transpose(mat)\n",
    "c = np.mean(mat,axis =0)\n",
    "\n",
    "d = (mat - c)\n",
    "v = np.transpose(d)\n",
    "f = np.matmul(v,d)\n",
    "f/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a7a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = (np.mean(x2*x2) - np.mean(x2)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(1.6669)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a89e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SD = np.array([[[99, 83, 22, 85],\n",
    "[86, 64, 30, 26],\n",
    "[54, 98, 18, 92],\n",
    "[63, 63, 38, 74]],\n",
    "[[48, 57, 18, 88],\n",
    "[84, 44, 13, 44],\n",
    "[73, 65, 38, 45],\n",
    "[89, 68, 44, 49]],\n",
    "[[75, 77, 97, 44],\n",
    "[88, 69, 50, 68],\n",
    "[64, 59, 53, 45],\n",
    "[34, 42, 92, 86]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97795e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 13, 38, 44])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD[1,0:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf69a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "89//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [10, 20]:\n",
    "    for j in [1, 2]:\n",
    "        print(i, j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eafd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "12345//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "12345%10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "44//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7afbf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [123,45,67,89,90,11,34,5678,98756]\n",
    "prm = []\n",
    "for nums in num:\n",
    "    for i in range(2,nums//2):\n",
    "        if nums%i == 0:\n",
    "           \n",
    "            break\n",
    "    else :\n",
    "        prm.append(nums)\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c344d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enetr a number1234567891\n"
     ]
    }
   ],
   "source": [
    "txt = int(input('enetr a number'))\n",
    "total = 0\n",
    "quo = 10000\n",
    "while quo != 0:\n",
    "    quo = txt//10\n",
    "    rem = txt%10\n",
    "    total += rem\n",
    "    txt = quo\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925e8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7917432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  123    90 98756]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([123,45,67,89,90,11,34,5678,98756])\n",
    "print(arr[::4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6473685d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trunc(3.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803e93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
